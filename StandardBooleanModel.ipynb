{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Standard Boolean Model\n",
        "\n",
        "The standard Boolean model, introduced in the 1970s, is a classic text retrieval method. Back then, computing power was limited, and data was stored on sequential tape drives. This model uses Boolean logic (AND, OR, NOT) to search for documents. Documents and queries are treated as **sets of words** and the model helps find documents by matching these sets based on Boolean operations. Despite more advanced methods being developed, the Boolean model is still used because it remains effective.\n",
        "\n",
        "We will create a standard Boolean model that performs the following operations:\n",
        "\n",
        "- ùëÑ = ùë°: The term ùë° must be present.\n",
        "- ùëÑ = ¬¨ùë°: The term ùë° must not be present.\n",
        "- ùëÑ = ùëÑ1 ‚à® ùëÑ2: Either sub-query ùëÑ1 or sub-query ùëÑ2 must be satisfied.\n",
        "- ùëÑ = ùëÑ1 ‚àß ùëÑ2: Both sub-query ùëÑ1 and sub-query ùëÑ2 must be satisfied.\n",
        "\n",
        "To simplify, we will focus on using only one operation at a time. Our code will not support multiple or mixed operations."
      ],
      "metadata": {
        "id": "MXOBUl6LLksj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need a list where we can store all the extract text from pdf\n",
        "documents = []"
      ],
      "metadata": {
        "id": "oCsht2FU0ETw"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from PDF files\n",
        "import pdfplumber\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "docs_collection = ['computer_science.pdf', 'physics.pdf', 'art.pdf',\n",
        "                   'football_history.pdf', 'artificial_intelligence.pdf']\n",
        "\n",
        "\n",
        "for path in docs_collection:\n",
        "  # For every document in the collection we fetch the text and store\n",
        "  doc = extract_text_from_pdf(path)\n",
        "  documents.append(doc)\n"
      ],
      "metadata": {
        "id": "6WhUEJ7N0CA4"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Load spaCy model\n",
        "# Load the spaCy small English model.\n",
        "# You can also use the larger model \"en_core_web_lg\" for better accuracy, but it is more expensive.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 2: Preprocess and create a vocabulary\n",
        "vocab = set()  # Set to store unique vocabulary terms\n",
        "preprocessed_docs = []  # List to hold preprocessed documents\n",
        "\n",
        "for doc in documents:\n",
        "    # Apply spaCy NLP pipeline to the document\n",
        "    doc_nlp = nlp(doc)\n",
        "    # Filter and lemmatize tokens, excluding stop words and certain POS tags\n",
        "    filtered_tokens = [\n",
        "        token.lemma_.upper() for token in doc_nlp\n",
        "        if not token.is_stop and token.pos_ not in {\"DET\", \"ADP\", \"AUX\", \"CCONJ\", \"ADJ\", \"PUNCT\", \"SPACE\"}\n",
        "    ]\n",
        "    # Update vocabulary set with filtered tokens\n",
        "    # Vocab keeps only unique tokens\n",
        "    vocab.update(filtered_tokens)\n",
        "    preprocessed_docs.append(filtered_tokens)  # Append filtered tokens to preprocessed documents list\n",
        "\n",
        "# Sorted list for consistent vector representation\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "# Step 3: Convert documents to Boolean vectors\n",
        "doc_vectors = []  # List to hold document vectors\n",
        "\n",
        "for tokens in preprocessed_docs:\n",
        "    # Create Boolean vectors for each document, if term is present in vocabulary\n",
        "    doc_vectors.append([1 if term in tokens else 0 for term in vocab])\n",
        "\n",
        "def search_docs(query, doc_vectors, vocab):\n",
        "    # Preprocess and split query into terms and operator\n",
        "    full_query = query.upper().split()\n",
        "    # Keep only the terms\n",
        "    terms = [t for t in full_query if t not in ['AND', 'OR', 'NOT']]\n",
        "    # Get the operation used in the query\n",
        "    operation = next((t for t in full_query if t in ['AND', 'OR', 'NOT']), None)\n",
        "    # Create Boolean vector for the query\n",
        "    query_vector = [1 if token in terms else 0 for token in vocab]\n",
        "\n",
        "    if full_query:\n",
        "        for doc_index, doc_vec in enumerate(doc_vectors):\n",
        "            if operation == 'AND':\n",
        "                # All 1s in the query vector match the corresponding 1s in the document vector\n",
        "                match = all(q == d for q, d in zip(query_vector, doc_vec) if q == 1)\n",
        "            elif operation == 'NOT':\n",
        "                # All 1s in the query vector are not present in the document vector\n",
        "                match = all(q != d for q, d in zip(query_vector, doc_vec) if q == 1)\n",
        "            # This can be used for OR operation, but also when there is no operation\n",
        "            else:\n",
        "                # Check if any 1s in the query vector are present in the document vector\n",
        "                match = any(q == d for q, d in zip(query_vector, doc_vec) if q == 1)\n",
        "\n",
        "            # Print document index if it matches the query\n",
        "            if match:\n",
        "                print(f'Doc: {doc_index}')\n"
      ],
      "metadata": {
        "id": "Ir6yXF_ejpqH"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "Let‚Äôs test different queries and see the results."
      ],
      "metadata": {
        "id": "2F-KmudKhn-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"ai\"\n",
        "\n",
        "search_docs(query, doc_vectors, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj-AL_AgT5mi",
        "outputId": "e30ea5fd-41a6-451f-87ca-949bf775b02a"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc: 0\n",
            "Doc: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"not quantum\"\n",
        "\n",
        "search_docs(query, doc_vectors, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLvdYXIrT8Af",
        "outputId": "800a53f0-29b2-4454-b7ef-b51f3103a12a"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc: 2\n",
            "Doc: 3\n",
            "Doc: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"art and physics\"\n",
        "\n",
        "search_docs(query, doc_vectors, vocab)"
      ],
      "metadata": {
        "id": "qFX0jxIZUAai"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"art or physics\"\n",
        "\n",
        "search_docs(query, doc_vectors, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOW5aOOTUGBl",
        "outputId": "4dc37e38-3498-475e-a90b-8dd4aa48acc1"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc: 1\n",
            "Doc: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "All the queries returned accurate results. If we check the PDF documents, we can see that the search effectively finds relevant documents.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- **Simplicity**: The Standard Boolean Model is straightforward, offering a clear and intuitive description of query semantics.\n",
        "- **Ease of Implementation**: It is easy to implement and understand, making it user-friendly.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "- **Lack of Control**: There is limited control over the number of retrieved documents, which can lead to either too few or too many results.\n",
        "- **Binary Nature**: The model only returns documents that strictly match the query terms, which may not always align with user expectations."
      ],
      "metadata": {
        "id": "GAo2mGyqlygk"
      }
    }
  ]
}