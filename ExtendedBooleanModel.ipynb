{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFpPWuC_2fgB"
      },
      "source": [
        "# Extended Boolean Model\n",
        "\n",
        "The standard Boolean model lacks ranking, which limits its effectiveness. The Extended Boolean Model solves this by incorporating term weights, using a bag of words approach, and allowing partial matching. This lets us return similarity scores rather than just true/false results, making document retrieval more flexible and accurate.\n",
        "\n",
        "We'll use the Fuzzy Algebraic technique (only works for two operands) to improve search results.\n",
        "\n",
        "- ð‘ ð‘–ð‘š(ð‘„1 âˆ§ ð‘„2, ð·ð‘–) = ð‘ ð‘–ð‘š(ð‘„1, ð·ð‘–) x ð‘ ð‘–ð‘š(ð‘„2, ð·ð‘–)\n",
        "\n",
        "- ð‘ ð‘–ð‘š (ð‘„1 âˆ¨ ð‘„2, ð·ð‘–) = ð‘ ð‘–ð‘š(ð‘„1, ð·ð‘–) + ð‘ ð‘–ð‘š(ð‘„2, ð·ð‘–) - ð‘ ð‘–ð‘š(ð‘„1, ð·ð‘–) x ð‘ ð‘–ð‘š(ð‘„2, ð·ð‘–)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WWqRZuFzI2GR"
      },
      "outputs": [],
      "source": [
        "# We need a list where we can store all the extract text from pdf\n",
        "documents = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1zqAnhlVAZWE"
      },
      "outputs": [],
      "source": [
        "# Extract text from PDF files\n",
        "import pdfplumber\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "docs_collection = ['computer_science.pdf', 'physics.pdf', 'art.pdf',\n",
        "                   'football_history.pdf', 'artificial_intelligence.pdf']\n",
        "\n",
        "\n",
        "for path in docs_collection:\n",
        "  # For every document in the collection we fetch the text and store\n",
        "  doc = extract_text_from_pdf(path)\n",
        "  documents.append(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "7chbGsejIzLS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Load spaCy model\n",
        "# You can also use the larger model \"en_core_web_lg\" for better accuracy, but it is more expensive.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 2: Preprocess and create a vocabulary\n",
        "vocab = set()  # Set to store unique vocabulary terms\n",
        "preprocessed_docs = []  # List to hold preprocessed documents\n",
        "\n",
        "for doc in documents:\n",
        "    # Apply spaCy NLP pipeline to the document\n",
        "    doc_nlp = nlp(doc)\n",
        "    # Filter and lemmatize tokens, excluding stop words and certain POS tags\n",
        "    filtered_tokens = [\n",
        "        token.lemma_.upper() for token in doc_nlp\n",
        "        if not token.is_stop and token.pos_ not in {\"DET\", \"ADP\", \"AUX\", \"CCONJ\", \"ADJ\", \"PUNCT\", \"SPACE\"}\n",
        "    ]\n",
        "    # Update vocabulary set with filtered tokens\n",
        "    # Vocab keeps only unique tokens\n",
        "    vocab.update(filtered_tokens)\n",
        "    preprocessed_docs.append(filtered_tokens)  # Append filtered tokens to preprocessed documents list\n",
        "\n",
        "# Sorted list for consistent vector representation\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "# Step 3: Convert documents to Boolean vectors\n",
        "doc_vectors = []\n",
        "\n",
        "for tokens in preprocessed_docs:\n",
        "    # Using bag of words\n",
        "    # The vector stores how often each vocab term appears in the document\n",
        "    # Example:\n",
        "    # Document (tokens): ['ai', 'art', 'ai', 'sport']\n",
        "    # Vocabulary (vocab): ['ai', 'physics', 'math', 'art', 'sport']\n",
        "    # Resulting doc_vector: [2, 0, 0, 1, 1]\n",
        "    doc_vectors.append([tokens.count(term) for term in vocab])\n",
        "\n",
        "\n",
        "def get_query_vector(term, vocab):\n",
        "    # Generate a binary query vector for the given term\n",
        "    # 1 if term is in query else 0\n",
        "    return [1 if token == term else 0 for token in vocab]\n",
        "\n",
        "def calculate_similarity(query_vector, doc_vec):\n",
        "    # Calculate similarity score for a given query vector and document vector\n",
        "    # For every doc where query match:\n",
        "    # Return: (Sum of the term frequency) / (Total tokens in doc)\n",
        "    return sum(doc_vec[i] for i, val in enumerate(query_vector) if val == 1) / len(doc_vec)\n",
        "\n",
        "\n",
        "def search_docs(query, doc_vectors, vocab):\n",
        "    # Preprocess query to split into terms and operand\n",
        "    query = query.upper().split()\n",
        "    # Get only terms\n",
        "    terms = [t for t in query if t not in {'AND', 'OR'}]\n",
        "    # Get only the operand\n",
        "    operand = next((t for t in query if t in {'AND', 'OR'}), None)\n",
        "\n",
        "    # Generate binary vectors for query terms\n",
        "    query_vectors = [get_query_vector(term, vocab) for term in terms]\n",
        "\n",
        "    relevant_docs = []\n",
        "\n",
        "    for doc_index, doc_vec in enumerate(doc_vectors):\n",
        "      # First we calculate similarities\n",
        "      # Note: The second value might be None if the query has no operands.\n",
        "      sim1 = calculate_similarity(query_vectors[0], doc_vec)\n",
        "      sim2 = calculate_similarity(query_vectors[1], doc_vec) if len(query_vectors) > 1 else None\n",
        "\n",
        "      if operand == 'AND' and sim2 is not None:\n",
        "        # score = ð‘ ð‘–ð‘š(ð‘„1 âˆ§ ð‘„2, ð·ð‘–)\n",
        "        # ð‘ ð‘–ð‘š(ð‘„1 âˆ§ ð‘„2, ð·ð‘–) = ð‘ ð‘–ð‘š(ð‘„1, ð·ð‘–) x ð‘ ð‘–ð‘š(ð‘„2, ð·ð‘–)\n",
        "        score = sim1 * sim2\n",
        "      elif operand == 'OR' and sim2 is not None:\n",
        "        # score = ð‘ ð‘–ð‘š(ð‘„1 âˆ¨ ð‘„2, ð·ð‘–)\n",
        "        # ð‘ ð‘–ð‘š(ð‘„1 âˆ¨ ð‘„2, ð·ð‘–) = ð‘ ð‘–ð‘š(ð‘„1, ð·ð‘–) + ð‘ ð‘–ð‘š(ð‘„2, ð·ð‘–) - ð‘ ð‘–ð‘š(ð‘„1, ð·ð‘–) x ð‘ ð‘–ð‘š(ð‘„2, ð·ð‘–)\n",
        "        score = sim1 + sim2 - sim1 * sim2\n",
        "      else:\n",
        "        # If no operands in the query\n",
        "        score = sim1\n",
        "\n",
        "      # Store only positive scores\n",
        "      if score > 0:\n",
        "          relevant_docs.append({'doc_index': doc_index, 'doc_score': score})\n",
        "\n",
        "    # Sort and output relevant documents\n",
        "    relevant_docs.sort(key=lambda x: x['doc_score'], reverse=True)\n",
        "\n",
        "    # Show relevant documents\n",
        "    for doc in relevant_docs:\n",
        "        print(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLPe2MTebcOp",
        "outputId": "8c641057-431c-4ff3-9599-d2d626372bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 1, 'doc_score': 8.007647303174531e-05}\n",
            "{'doc_index': 0, 'doc_score': 1.5014338693452247e-05}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"quantum and technology\", doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDTV-Fd_B-jM",
        "outputId": "cc41e068-ce80-4684-b45b-fa12efc1a803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 1, 'doc_score': 0.037951243437482796}\n",
            "{'doc_index': 0, 'doc_score': 0.008933531522604087}\n",
            "{'doc_index': 3, 'doc_score': 0.0044742729306487695}\n",
            "{'doc_index': 4, 'doc_score': 0.0044742729306487695}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"technology or quantum\", doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MijJOa6Dngds",
        "outputId": "a63f0aa4-6607-4391-baea-f05ac6f8d08a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 0, 'doc_score': 0.006711409395973154}\n",
            "{'doc_index': 3, 'doc_score': 0.0044742729306487695}\n",
            "{'doc_index': 4, 'doc_score': 0.0044742729306487695}\n",
            "{'doc_index': 1, 'doc_score': 0.0022371364653243847}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"technology\", doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ji_fqUbS5Wq",
        "outputId": "df040809-b488-48a6-9d57-f629a8008ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 2, 'doc_score': 0.015659955257270694}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"art\", doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAo2mGyqlygk"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "All the queries returned accurate results. If we check the PDF documents, we can confirm that the search effectively identifies and ranks the most relevant documents.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- Provides ranked results and partial matches, enhancing user control over result presentation.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "- Users might struggle with complex queries using simple AND/OR combinations.\n",
        "\n",
        "\n",
        "We have only explored Fuzzy Algebraic, but several variants of the Extended Boolean Model exist for calculating AND and OR operators, including Fuzzy Set, Soft Boolean Operator, Paice Model, and P-Norm Model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
