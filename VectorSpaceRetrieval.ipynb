{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqCxDfavGWCE"
      },
      "source": [
        "# Vector Space Retrieval\n",
        "\n",
        "Vector Space Retrieval in text retrieval converts documents and queries into vectors, where each term represents a dimension. The frequency of terms in a document (e.g., TF-IDF) determines the value for each vector dimension. To find relevant documents, the system calculates the similarity between the query vector and document vectors, usually using cosine similarity. The documents with the highest similarity scores are considered the most relevant. This method enables ranked retrieval by measuring how close vectors are, rather than relying solely on exact matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "e3kLQtSP_mkt"
      },
      "outputs": [],
      "source": [
        "# We need a list where we can store all the extract text from pdf\n",
        "documents = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jHtIbSBw_aQ5"
      },
      "outputs": [],
      "source": [
        "# Extract text from PDF files\n",
        "import pdfplumber\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "docs_collection = ['computer_science.pdf', 'physics.pdf', 'art.pdf',\n",
        "                   'football_history.pdf', 'artificial_intelligence.pdf']\n",
        "\n",
        "\n",
        "for path in docs_collection:\n",
        "  # For every document in the collection we fetch the text and store\n",
        "  doc = extract_text_from_pdf(path)\n",
        "  documents.append(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kb-GiuaCbPob"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 2: Preprocess and create a vocabulary\n",
        "vocab = set()\n",
        "preprocessed_docs = []\n",
        "\n",
        "for doc in documents:\n",
        "    # Apply spaCy NLP pipeline to the document\n",
        "    doc_nlp = nlp(doc)\n",
        "    # Filter and lemmatize tokens, excluding stop words and certain POS tags\n",
        "    filtered_tokens = [\n",
        "        token.lemma_.upper() for token in doc_nlp\n",
        "        if not token.is_stop and token.pos_ not in {\"DET\", \"ADP\", \"AUX\", \"CCONJ\", \"ADJ\", \"PUNCT\", \"SPACE\"}\n",
        "    ]\n",
        "    # Update vocabulary set with filtered tokens\n",
        "    # Vocab keeps only unique tokens\n",
        "    vocab.update(filtered_tokens)\n",
        "    preprocessed_docs.append(filtered_tokens)  # Append filtered tokens to preprocessed documents list\n",
        "\n",
        "# Sorted list for consistent vector representation\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "# Step 3: Compute term frequency (TF)\n",
        "def compute_tf(tokens, vocab):\n",
        "    term_counts = Counter(tokens)\n",
        "    # Number of times term t appears in document d / total number of terms in the doc d\n",
        "    return np.array([term_counts[term] / len(tokens) for term in vocab])\n",
        "\n",
        "doc_vectors = [compute_tf(tokens, vocab) for tokens in preprocessed_docs]\n",
        "\n",
        "# Step 4: Compute inverse document frequency (IDF)\n",
        "def compute_idf(docs, vocab):\n",
        "    # log(total number of docs / number of docs containing term t)\n",
        "    idf = np.log(len(docs) / (1 + np.array([sum(1 for doc in docs if term in doc) for term in vocab])))\n",
        "    return idf\n",
        "\n",
        "idf = compute_idf(preprocessed_docs, vocab)\n",
        "tfidf_doc_vectors = [tf_vec * idf for tf_vec in doc_vectors]\n",
        "\n",
        "# Step 5: Create TF-IDF query vector\n",
        "def get_query_vector(query, vocab):\n",
        "    # Lemmatized tokens of the query\n",
        "    query_tokens = [token.lemma_.upper() for token in nlp(query)]\n",
        "    query_tf = compute_tf(query_tokens, vocab)\n",
        "    return query_tf * idf\n",
        "\n",
        "# Step 6: Calculate cosine similarity\n",
        "def calculate_similarity(query_vector, doc_vector):\n",
        "    return cosine_similarity([query_vector], [doc_vector])[0][0]\n",
        "\n",
        "# Step 7: Search documents using vector space model\n",
        "def search_docs(query, tfidf_doc_vectors, vocab):\n",
        "    query_vector = get_query_vector(query, vocab)\n",
        "    similarities = []\n",
        "\n",
        "    for doc_index, doc_vector in enumerate(tfidf_doc_vectors):\n",
        "        sim_score = calculate_similarity(query_vector, doc_vector)\n",
        "        if sim_score > 0:\n",
        "            similarities.append({'doc_index': doc_index, 'doc_score': sim_score})\n",
        "\n",
        "    similarities.sort(key=lambda x: x['doc_score'], reverse=True)\n",
        "\n",
        "    for doc in similarities:\n",
        "        print(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkj7isfObPmH",
        "outputId": "232a6cce-7e85-4e63-e398-0b8344280162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 1, 'doc_score': 0.5179971042532392}\n",
            "{'doc_index': 0, 'doc_score': 0.044226915215261814}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"quantum\", tfidf_doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47HyMRXEbPjw",
        "outputId": "81c06060-feca-4a87-b27d-e185c8cfc55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 3, 'doc_score': 0.05068685199546379}\n",
            "{'doc_index': 2, 'doc_score': 0.03368341028251341}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"history\", tfidf_doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqgIUdu8bPhs",
        "outputId": "b96d58c6-2ec4-4b49-caa3-dd3809f0e73d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 4, 'doc_score': 0.4716339957973389}\n",
            "{'doc_index': 0, 'doc_score': 0.13268074564578544}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"ai\", tfidf_doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j7wJkjYbPfG",
        "outputId": "f943eec0-10e4-4ae2-dcf8-382bb9c46d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 0, 'doc_score': 0.30641305685949793}\n",
            "{'doc_index': 4, 'doc_score': 0.14852618969049403}\n",
            "{'doc_index': 2, 'doc_score': 0.019447125993833726}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"science artificial intelligence\", tfidf_doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os9PHIyIF9Hg",
        "outputId": "7a77834e-9aa5-411c-caa5-64c4d4844821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doc_index': 4, 'doc_score': 0.18190668909076688}\n",
            "{'doc_index': 0, 'doc_score': 0.09381945497902239}\n"
          ]
        }
      ],
      "source": [
        "search_docs(\"artificial intelligence\", tfidf_doc_vectors, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gnlShSMSr6-"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "All the queries returned accurate results. If we check the PDF documents, we can confirm that the search effectively identifies and ranks the most relevant documents.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- Extreme simple an intuitive query model.\n",
        "- Allows partial matches (documents don't need all query terms)\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "- Can be biased by term spamming.\n",
        "- Assumes term independence, which isn't always true (e.g., synonyms)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BM25 (Best Matching 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have seen the effectiveness of Vector Space Retrieval, but it also comes with issues like bias from term spamming, which is something we want to avoid. For example, if a document has the term \"AI\" repeated 100 times, it might be ranked first due to the high term frequency, even if it's not truly the most relevant.\n",
        "\n",
        "To address this issue, we can use BM25, which is a ranking function designed to tackle two main problems with traditional TF-IDF:\n",
        "\n",
        "- **Diminishing returns on term frequency**: BM25 uses a saturation function so that repeating a term excessively doesn’t lead to an exaggerated boost in relevance.\n",
        "- **Document length normalization**: BM25 adjusts for document length, making sure shorter documents are not unfairly ranked lower than longer ones.\n",
        "This helps ensure that relevance scores are more balanced and reflective of actual content quality."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
